{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "psychological-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # neural network modules\n",
    "import torch.nn.functional as F  # activation functions\n",
    "import torch.optim as optim  # optimizer\n",
    "from torch.autograd import Variable # add gradients to tensors\n",
    "from torch.nn import Parameter # model parameter functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monthly-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spectacular-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df_orig = pd.read_csv(\"train.csv\")\n",
    "df = df_orig.iloc[:1200].reset_index(drop=True)\n",
    "\n",
    "# Fix target label\n",
    "label_encodings = {\n",
    "    'pants-fire': 0, \n",
    "    'false':      0, \n",
    "    'barely-true':1, \n",
    "    'half-true':  1, \n",
    "    'mostly-true':2,\n",
    "    'true':       2\n",
    "}\n",
    "df['target'] = df['label'].apply(lambda x: label_encodings[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-calendar",
   "metadata": {},
   "source": [
    "### Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "changing-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Glove file\n",
    "words = pd.read_table(\"glove.6B.100d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# Generate Glove dictionary\n",
    "glove = {word:words.iloc[idx].values for (word,idx) in zip(words.index,range(words.shape[0]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "civilian-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into words\n",
    "df['words'] = df['statement'].apply(lambda x: x.replace('?',' ?').replace('.',' .').\\\n",
    "                                    lower().split())\n",
    "\n",
    "# Generate the list of all vocab words in our data\n",
    "target_vocab = list(itertools.chain.from_iterable(df['words']))\n",
    "target_vocab = list(set(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "guilty-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 945 out of 4876 words not in the Glove model\n"
     ]
    }
   ],
   "source": [
    "# Generate the weights_matrix, which is the matrix of word embeddings that we\n",
    "# pass into Pytorch. It contains len(target_vocab) rows for each of the words,\n",
    "# each represented by a length-100 vector – taken from the Glove embedding\n",
    "weights_matrix = np.zeros((len(target_vocab)+1,100))\n",
    "\n",
    "# Add in vocab\n",
    "error_count = 0\n",
    "word_to_idx = {}\n",
    "for i, word in enumerate(target_vocab):\n",
    "    word_to_idx[word] = i\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(100,))\n",
    "        error_count += 1\n",
    "\n",
    "# Add in \"empty\" token\n",
    "word_to_idx[\"\"] = len(target_vocab)\n",
    "        \n",
    "print(f\"We found {error_count} out of {len(target_vocab)} words not in the Glove model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bizarre-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the text to have length 100\n",
    "df['words'] = df['words'].apply(lambda x: x+([\"\"]*(100-len(x))))\n",
    "\n",
    "# Encode text using the indices\n",
    "df['text_idx'] = df['words'].apply(lambda lst: np.array([word_to_idx[w] for w in lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "structured-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, \n",
    "                     non_trainable=False):\n",
    "    weights_matrix = torch.Tensor(weights_matrix)\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-india",
   "metadata": {},
   "source": [
    "### FakeBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "refined-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, targets):\n",
    "\n",
    "    predicted = [int(y_pred.detach().argmax(-1)) for y_pred in output]\n",
    "    targets = [int(y) for y in targets]\n",
    "    correct = sum(a==b for (a,b) in zip(predicted, targets))\n",
    "    accuracy = 100*correct/len(targets) \n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def train(data,\n",
    "          test,\n",
    "          weights_matrix,\n",
    "          num_epochs = 10,\n",
    "          batch_size = 100,\n",
    "          learning_rate = 0.01):\n",
    "    \n",
    "    # Instantiate model & optimization \n",
    "    model = FakeBERT(weights_matrix)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for ep in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # Iterate over batches\n",
    "        for i in range(data.shape[0]//batch_size):\n",
    "                            \n",
    "            # Declare features and target labels\n",
    "            X = np.array([i for i in data['text_idx'][i*batch_size:(i+1)*batch_size]])\n",
    "            y = data['target'][i*batch_size:(i+1)*batch_size].values\n",
    "            y = torch.Tensor(y).to(dtype=torch.long)\n",
    "\n",
    "            # Get predictions from model\n",
    "            pred = model(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(pred, y)\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        \n",
    "        # Prepare test data\n",
    "        test_X = np.array([i for i in test['text_idx']])\n",
    "        test_y = test['target'].values\n",
    "        test_y = torch.Tensor(test_y).to(dtype=torch.long)\n",
    "            \n",
    "        # Evaluate on test data\n",
    "        test_pred = model(test_X)\n",
    "        test_accuracy = get_accuracy(test_pred, test_y)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print(f\"Test accuracy: {test_accuracy} at epoch: {ep}\")\n",
    "\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "working-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeBERT(nn.Module):\n",
    "    def __init__(self, weights_matrix):\n",
    "        super(FakeBERT, self).__init__()\n",
    "        \n",
    "        # Layer 0: Embedding Layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "\n",
    "        # Layer 1: Conv1D + Maxpool\n",
    "        self.conv_1 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.sigm_1 = nn.ReLU()\n",
    "        self.pool_1 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 2: Conv1D + Maxpool\n",
    "        self.conv_2 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=4, stride=1)\n",
    "        self.sigm_2 = nn.ReLU()\n",
    "        self.pool_2 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 3: Conv1D + Maxpool\n",
    "        self.conv_3 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=5, stride=1)\n",
    "        self.sigm_3 = nn.ReLU()\n",
    "        self.pool_3 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 4: Conv1D + Maxpool\n",
    "        self.conv_4 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, stride=1)\n",
    "        self.sigm_4 = nn.ReLU()\n",
    "        self.pool_4 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 5: Conv1D + Maxpool\n",
    "        self.conv_5 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, stride=1)\n",
    "        self.sigm_5 = nn.ReLU()\n",
    "        self.pool_5 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 6: Fully Connected Layer \n",
    "        self.full_6 = nn.Linear(128,32)\n",
    "        self.sigm_6 = nn.Sigmoid()\n",
    "        \n",
    "        # Layer 7: Fully Connected Layer \n",
    "        self.full_7 = nn.Linear(32,3)\n",
    "        self.soft_7 = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate the embeddings with Glove\n",
    "        emb = self.embedding(torch.Tensor(x).to(dtype=torch.long))\n",
    "\n",
    "        # Generate the 3 1D conv layers\n",
    "        conv_1 = self.pool_1(self.sigm_1(self.conv_1(emb)))        \n",
    "        conv_2 = self.pool_2(self.sigm_2(self.conv_2(emb)))        \n",
    "        conv_3 = self.pool_3(self.sigm_3(self.conv_3(emb)))\n",
    "        \n",
    "        # Concatenate the 3 layers\n",
    "        cat = torch.cat((conv_1,conv_2,conv_3),2)\n",
    "        \n",
    "        # Pass the concatenated output through 2 1D conv layers\n",
    "        conv_4 = self.pool_4(self.sigm_4(self.conv_4(cat)))        \n",
    "        conv_5 = self.pool_5(self.sigm_5(self.conv_5(conv_4)))  \n",
    "\n",
    "        # Flatten the output\n",
    "        flat = conv_5.flatten(start_dim=1)\n",
    "\n",
    "        # Pass through 2 fully connected layers\n",
    "        full_6 = self.sigm_6(self.full_6(flat))\n",
    "        full_7 = self.soft_7(self.full_7(full_6))\n",
    "        \n",
    "        return full_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "defensive-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:1000].reset_index(drop=True)\n",
    "test = df.iloc[1000:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "alternate-mayor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 36.0 at epoch: 0\n",
      "Test accuracy: 36.0 at epoch: 1\n",
      "Test accuracy: 36.0 at epoch: 2\n",
      "Test accuracy: 36.0 at epoch: 3\n",
      "Test accuracy: 36.0 at epoch: 4\n",
      "Test accuracy: 36.0 at epoch: 5\n",
      "Test accuracy: 36.0 at epoch: 6\n",
      "Test accuracy: 36.0 at epoch: 7\n",
      "Test accuracy: 36.0 at epoch: 8\n",
      "Test accuracy: 36.0 at epoch: 9\n"
     ]
    }
   ],
   "source": [
    "test_data = train(data, test, weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-grass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
